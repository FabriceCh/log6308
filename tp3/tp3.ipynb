{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2\n",
    "### Équipe 10\n",
    "### Fabrice Charbonneau (1798064)\n",
    "### Sanou Armel Kenzanga Landry (1976804)\n",
    "\n",
    "\n",
    "1. créer une matrice termes-documents; pour diminuer la taille du vocabulaire, utiliser un stemmer et éliminer les chiffres et la ponctuation;\n",
    "2. transformer les fréquences brutes par les valeurs TF-IDF;\n",
    "3. effectuer une réduction de dimensions avec SVD; utilisez 50 dimensions latentes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'abord, lire les fichiers et les ajouter à un dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(\"raw_data.csv\"):\n",
    "    i = 0\n",
    "    df = pd.DataFrame(columns=['uni', 'course', 'text'])\n",
    "    class_data = 'Descriptions' \n",
    "    corpus = PlaintextCorpusReader(class_data, '.*txt', encoding='latin-1')\n",
    "    for file in corpus.fileids():\n",
    "        uni_str = file.split(\"/\")[0]\n",
    "        new_corpus = PlaintextCorpusReader(class_data, file, encoding='latin-1')\n",
    "        df.loc[i] = [uni_str, file.split(\"/\")[1].split(\".\")[0], new_corpus.raw()]\n",
    "        i += 1\n",
    "        print(i,  end='\\r')\n",
    "    df.to_csv(\"raw_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"raw_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Détection du langage des textes avec https://pypi.org/project/langdetect/\n",
    "\n",
    "Nous n'allons conserver que les cours ayant comme langage détecté le français."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "if not path.exists(\"fr_only.csv\"):\n",
    "    df[\"lang\"] = df.apply( lambda row: detect(row[\"text\"]), axis=1)\n",
    "    df = df[df[\"lang\"] == \"fr\"]\n",
    "    df.to_csv(\"fr_only.csv\", index=False)\n",
    "df = pd.read_csv(\"fr_only.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=\"lang\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation d'un Tokenizer pour séparer les strings en mots.\n",
    "\n",
    "TweetTokenizer de nltk est à la base fait pour les tweets, donc traite le texte pouvant contenir des hashtags et autres particularité des tweets. Dans notre cas, ça ne change pas grand chose, il fera facilement le travail même le texte n'est pas des tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "tokens = df[\"text\"].apply(lambda x: tknzr.tokenize(x)).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmer afin d'éliminer les tokens inutles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = SnowballStemmer(\"french\", ignore_stopwords=True)\n",
    "[st.stem(word) for word in [\"Yeux\", \"Sociologie\", \".\", \"souris\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, stem=True, punctuation=True, digit=True):\n",
    "        self.st = SnowballStemmer(\"french\", ignore_stopwords=True)\n",
    "        self.punctuations = set(string.punctuation)\n",
    "        self.stem = stem\n",
    "        self.punctuation = punctuation\n",
    "        self.digit = digit\n",
    "    \n",
    "    def raw_preprocess(self, token_list):\n",
    "        new_token_list = []\n",
    "        for token in token_list:\n",
    "            is_token_valid = True\n",
    "            token = token.lower()\n",
    "            \n",
    "            # stemming\n",
    "            if self.stem:\n",
    "                token = self.st.stem(token)\n",
    "\n",
    "            # punctuation filtering\n",
    "            if self.punctuation and token in self.punctuations:\n",
    "                is_token_valid = False\n",
    "\n",
    "            # digit filtering\n",
    "            elif self.digit and token.isdigit():\n",
    "                is_token_valid = False\n",
    "\n",
    "            # add token to new sub list\n",
    "            if is_token_valid:\n",
    "                new_token_list.append(token)\n",
    "        return new_token_list\n",
    "        \n",
    "    def preprocess_tokens(self, tokens):\n",
    "        new_tokens = []\n",
    "        for token_list in tokens:\n",
    "            new_tokens.append(self.raw_preprocess(token_list))\n",
    "        return new_tokens\n",
    "    \n",
    "def preprocess(tokens):\n",
    "    preprocessor = Preprocessor()\n",
    "    return preprocessor.preprocess_tokens(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [\n",
    "    ['Sociologie', 'de', 'l', 'DescriptionCours', ':', 'Trois', 'volets', 'principaux', 'dans', 'ce', 'cours'],\n",
    "    ['Management', 'DescriptionCours', ':', 'Ce', 'cours', 'est', 'une', 'introduction', 'au', 'management', 'et']\n",
    "]\n",
    "preprocessor = Preprocessor()\n",
    "preproc_test = preprocessor.preprocess_tokens(test)\n",
    "preproc_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def identity_tokenizer(text):\n",
    "    return text\n",
    "\n",
    "#vectorizer = TfidfVectorizer(preprocessor = preprocessor.preprocess_tokens, tokenizer=identity_tokenizer)\n",
    "vectorizer = TfidfVectorizer(preprocessor=preprocessor.raw_preprocess ,tokenizer=identity_tokenizer)\n",
    "IDF_data = vectorizer.fit_transform(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=50, algorithm='randomized', n_iter=100)\n",
    "svd.fit(IDF_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(svd.explained_variance_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVD_data = svd.transform(IDF_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quels sont les 10 cours les plus similaires à LOG2420 dans l'espace réduit à 50 dimensions? (8 pts.) Prenez le cosinus comme mesure de similarité. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG2420_idx = df.index[df[\"course\"] == 'LOG2420'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "cos_matrix = cosine_similarity(SVD_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_2420_cos = cos_matrix[LOG2420_idx]\n",
    "log_2420_sorted_cos = np.sort(log_2420_cos)\n",
    "recommendations_cos = log_2420_sorted_cos[::-1][1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations_indx = [np.where(log_2420_cos == reco)[0][0] for reco in recommendations_cos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[list(recommendations_indx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Effectuez une classification de cours par une approche supervisée (4 pts.) Pour établir les libellés de classes, prenez les acronymes de cours commençant par PSY (psychologie) et PHY (physique). Calculez les centroides de chaque classe (PSY et PHY) des données d'entraînement et calculez l'aire sous la courbe ROC pour la prédiction de données de test (AUC, Area Under receiver operator Curve). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ((df[\"course\"].astype(str).str[:3] == \"PSY\") | (df[\"course\"].astype(str).str[:3] == \"PHY\"))\n",
    "train_df = df[mask]\n",
    "\n",
    "train_df[\"PSY\"] = df[\"course\"].astype(str).str[:3] == \"PSY\"\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. À partir de la validation croisée de la tâche de classification ci-dessus, déterminez le nombre de dimensions latentes optimal de SVD selon une approche dite wrapper (4 pts.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Effectuez une agglomération par k-means (k=2) et vérifiez si les classes PHY et PSY sont bien séparées par cette méthode (4 pts.). (à compléter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
